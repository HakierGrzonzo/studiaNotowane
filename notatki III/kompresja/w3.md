#  Źródła informacji

**Ilością informacji** zdarzenia $x$ pojawiającego się z prawdopodobieństwem $p$ nazywamy liczbę:
$$
I_m (x) = - \log_m p
$$

Nie ma znaczenia, jakie $m$ przyjmiemy:
- Jeżeli $m=2$ to ilość informacji mierzymy w bitach.
- Jak $m=10$ to ilość informacji mierzymy w hartley'ach.
- Jak $m=e$ to ilość informacji mierzymy w nat.'ach

$m$ Będziemy uzależniać od tego ile elementów ma zbiór $B$ w kodzie $A \rightarrow B^*$.

> Jeżeli nie będziemy pisać podstawy w $\log$ to oznacza na zajęciach, że jest ona równa 2 (nie 10).

Wykres tej funkcji wygląda tak:

![[Pasted image 20211029083405.png]]

Ilość informacji również jest zwana jej wartością.

## Źródła

Źródłem nazywamy parę $(A, P)$ taką, że $A$ jest pewnym alfabetem, $P$ jest zbiorem prawdopodobieństw $(p_i \in [0, 1])$ takich, że:

$$
\sum^n_{i=1} p_i = 1
$$

> Zawsze źródło wygeneruje jakąś informację,
> Różne źródła mogą wypracować te same zbiory prawdopodobieństw

$a_i \rightarrow p_i$ - litera $a_i$ pojawia się z prawdopodobieństwem $p_i$.

## Średnia ilość informacji

Średnią ilością informacji nadawanych przez źródło $(A, P)$ nazywamy liczbę:
$$
H_m(A) = \sum^n_{i=1} p_i \log_m \frac{1}{p_i}
$$

Nazywana jest ona również *entropią źródła* lub *średnią niepewnością obserwatora przed pojawieniem się informacji ze źródła*.


### Przykład:

$A = \{a, b\}, P = \{p, q\}\quad q = 1 - p$
$H_m(p) = p\log_m\frac{1}{q} + (1-p)\log_m\frac{1}{1-p}$

Wykres wygląda wtedy tak:
![[Pasted image 20211029090428.png]]
Oś Y to entropia, dla $\frac{1}{2}$ jest maksymalna.

---
$$
A = \{a_1, \dots, a_n\}\quad p_1 = \dots = p_n = \frac{1}{n}
$$
$$
H_m(A) = \sum^n_{i=1} \frac{1}{n}\log_m n = \log_m n
$$

Zatem dla każdego układu entropia spełnia warunki:
- $0 \leq H_m(A)$
- $H_m(A) \leq \log_m n$, równość zachodzi tylko wtedy, gdy $p_1 = \dots = p_n$

### Uzasadnienie:

1. $\ln x \leq x -1$
2. Dla $p_1,\dots, p_n$ oraz $q_1,\dots, q_n$, takich, że $\sum^n_{i=1} p_i = \sum^n_{i=1} q_i = 1$ wtedy: $\sum^n_{i=1} p_i \log_m \frac{1}{p_i} \leq \sum^n_{i=1} p_i \log_m \frac{1}{q_i}$.

## Źródła potęgowe

Dzielimy je na źródła bez pamięci oraz na źródła z pamięcią.

$A = \{a_1, \dots,, a_n\}, p = \{p_1, \dots, p_n\}$

Jakie jest prawdopodobieństwo wystąpienia danego **Ciągu** informacji.

Źródło jest bez pamięci, jeżeli prawdopodobieństwo pojawienia się danego znaku **nie zależy** od znaków kolejnych (zdarzenia są niezależne).

> Zakładamy, że wszystkie omawiane źródła są bez pamięci.

> Polski jest źródłem z pamięcią.

> Istnieją jeszcze źródła *Markova*

---
Niech: $(A^2, p^2)$, gdzie $A= \{a_i a_j: a_i, a_j \in A\}$, $P^2 = \{p_i p_j: i, j \in \{1, \dots, n\}\}$

Zatem:
$$
\sum^n_{i=1}\sum^n_{j=1} p_i p_j = \sum^n_{i=1} p_i \underbrace{\left(\sum^n_{j=1} p_j\right)}_{=1} = 
\sum^n_{i=1} p_i = 1
$$

### Entropia źródeł potęgowych:

$$
\begin{align}
H_m(A^2) &= \sum^n_{i=1}\sum^n_{j=1} p_i p_j \log_m \frac{1}{p_i p_j} = \\ 
&= \sum^n_{i=1} p_i \sum^n_{j=1} p_j \left( \log_m \frac{1}{p_i} + \log_m \frac{1}{p_j}\right) = \\
&= \sum^n_{i=1} p_i \left( \sum^n_{j=1} \log_m \frac{1}{p_1} + \underbrace{\sum^n_{j=1} p_j \log_m \frac{1}{p_j}}_{H_m(A)}\right) = \\
&= \sum^n_{i=1} p_i \log_m \frac{1}{p_i} + H_m(A) = 
Z H_m(A)
\end{align}
$$

Dane są źródło $(A, P)$, oraz alfabet $B$ i kod j.d.
$$
f: A \rightarrow B^*
$$
Średnią długością słów kodowych nazywamy liczbę:
$$
L(f) = \sum^n_{i=1} p_i l_i
$$
Gdzie:
- $l_i$ - długość słowa $f(a_i)$ dla $i = 1, \dots, n$


---

### Kod optymalny

Dane są źródło $(A, P)$, oraz alfabet $B$ i kod j.d.
$$
f: A \rightarrow B^*
$$

Kod $f$ nazywamy kodem optymalnym, jeśli dla dowolnego kodu j.d. $f': A \rightarrow B^*$:

$$
L(f) \leq L(f')
$$